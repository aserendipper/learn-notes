尚硅谷实时数据仓库项目(阿里云实时数仓)
地址：[https://www.bilibili.com/video/BV1dJ411k7BE?p=1](https://www.bilibili.com/video/BV1dJ411k7BE?p=1)
# 第1章 项目需求及架构设计
## 1.1 项目需求分析
* 实时采集埋点日志数据
* 实时采集业务数据库中数据
* 对数据进行清洗和处理
* 保存数据到分析型数据库
* 对结果进行可视化展示

## 1.2 项目框架 
### 1.2.1 阿里云技术框架
* 阿里云产品->简介->类比
* ECS->弹性服务器->Linux服务器
* RDS->关系型数据库->Mysql
* DataHub->数据总线->Kafka+各种服务接口
* 实时计算->实时计算->Spark、Flink
* DataWorks（Stream Studio）->可视化StreamCompute的开发管理平台->目前没有
* AnalyticDB for Mysql->分析型数据库->Mysql集群
* DataV->可视化数据展示工具->Tableau、Echarts、Kibana
### 1.2.2 技术选型
* 阿里云框架->开源框架
* 数据采集传输：DataHub、DTS->Flume、Kafka、Canal、MaxWell
* 数据存储：RDS、AnalyticDB->Mysql、Hadoop、HBase
* 数据计算：实时计算->Spark、Flink
* 数据可视化：DataV、QuickBI->Tableau、Echarts、Kibana
### 1.2.3 系统架构设计
![Image text](image/尚硅谷实时数据仓库项目(阿里云实时数仓)/1.png)
* 埋点用户行为数据以文件形式存储，采用Flume读取并写入到DataHub
* RDS业务数据以mysql形式存储，分为DTS（同步事实表）和DTS（同步维表）
* DTS（同步事实表）数据同步到DataHub中，数据特点：需要初步加工进行join操作
* DTS（同步维表）数据同步到维表库RDS中，数据特点：不需要加工
* DataWorks把DataHub和维表库RDS中的数据汇总加工，然后写入到AnalyticDB，最后用DataV进行展示
### 1.2.4 业务流程
![Image text](image/尚硅谷实时数据仓库项目(阿里云实时数仓)/2.png)
## 1.3 电商表结构
![Image text](image/尚硅谷实时数据仓库项目(阿里云实时数仓)/3.png)
# 第2章 业务数据准备
## 2.1 RDS购买
阿里云关系型数据库（Relational Database Service，简称RDS）是一种稳定可靠、可弹性伸缩的在线数据库服务。
* 购买RDS for Mysql服务器地址：[https://www.aliyun.com/product/rds/mysql](https://www.aliyun.com/product/rds/mysql)

## 2.2 RDS配置
* 设置白名单，允许哪些网站可以访问RDS，配置本机IP
* 申请RDS的外网地址
## 2.3 RDS连接
* 账号管理->创建账号
* 测试服务器连接
## 2.4 创建业务数据库及表
* 创建gmall数据库
* 导入gmall_aliyun.sql文件
* 调用存储过程init_data生成测试数据 
# 第3章 缓冲数据
## 3.1 DataHub简介
DataHub类似于传统大数据解决方案中kafka的角色，提供了一个数据队列功能。
DataHub提供了各种与其他阿里云上下游产品的对接功能，所以DataHub除了提供一个缓冲队列的作用还扮演了一个数据分发枢纽工作。
![Image text](image/尚硅谷实时数据仓库项目(阿里云实时数仓)/4.png)

DataHub输入组件包括
* Flume：主流的开源日志采集框架
* DTS：类似Canal，日志实时监控采集框架
* Logstash：日志采集框架，通常和Elasticsearch、kibana集合使用
* Fluentd：Fluentd是一个实时开源的数据收集器
* OGG：实时监控Oracle中数据变化
* Java Sdk：支持JavaAPI方式访问

DataHub 输出组件包括
* RDS：类似与传统MySQL数据库
* AnalyticDB：面向分析型的分布式框架
* MaxCompute：离线分析框架
* Elasticsearch：查询引擎、数据分析、倒排索引
* StreamCompute：实时分析框架
* TableSotre：类似于 Redis，KV 形式存储数据
* OSS：类似于HDFS，存储图片、视频

## 3.2 创建DataHub及Project
阿里云 DataHub 控制台入口:
[https://www.aliyun.com/product/datahub?spm=5176.19720258.J_8058803260.51.73452c4aiH6dti](https://www.aliyun.com/product/datahub?spm=5176.19720258.J_8058803260.51.73452c4aiH6dti)
* 开通DataHub
* 新建项目gmall_datahub

# 第4章 同步业务数据
## 4.1 DTS同步数据
数据传输服务（Data Transmission Service）DTS支持关系型数据库、NoSQL、大数据（OLAP）等数据源间的数据传输。它是一种集数据迁移、数据订阅及数据实时同步于一体的数据传输服务。数据传输致力于在公共云、混合云场景下，解决远距离、毫秒级异步数据传输难题。它底层的数据流基础设施为阿里双11异地多活基础架构，为数千下游应用提供实时数据流，已在线上稳定运行5年之久。您可以使用数据传输轻松构建安全、可扩展、高可用的数据架构。
![Image text](image/尚硅谷实时数据仓库项目(阿里云实时数仓)/5.png)

## 4.2 同步事实表到DataHub
DTS购买地址：[https://www.aliyun.com/product/dts?spm=5176.12825654.eofdhaal5.66.54212c4aM9dufx](https://www.aliyun.com/product/dts?spm=5176.12825654.eofdhaal5.66.54212c4aM9dufx)
* 购买DTS
* 配置同步链路
* 选择同步对象
* 预检测
* 去 DataHub 中检查
* 在 gmall 数据库中生成部分数据
* 观察 DataHub 对应主题中有数据

## 4.3 同步维表到MySQL
* 创建gmall_dim数据库
* 创建DataHub同步作业
* 配置同步维表的 DTS
* 去 SQLyog 中检查
* 建立视图
    * 在维表库中建立省份和地区视图 
    ![Image text](image/尚硅谷实时数据仓库项目(阿里云实时数仓)/6.png)
    * 在维表库中建立商品和商品分类视图
    ![Image text](image/尚硅谷实时数据仓库项目(阿里云实时数仓)/7.png)

```
CREATE VIEW `dim_province` AS
SELECT
`bp`.`id`
`bp`.`name`
`br`.`id`
`br`.`region_name` AS `region_name`, `bp`.`area_code` AS`area_code`
FROM `base_region` `br` 
JOIN `base_province` `bp` 
ON `br`.`id` = `bp`.`region_id`
```

```
CREATE VIEW `dim_sku_info` AS
SELECT
`si`.`id` AS `id`, `si`.`sku_name` AS `sku_name`, `si`.`category3_id` AS `c3_id`,
`si`.`weight`
   `si`.`tm_id`
   `si`.`price`
   `si`.`spu_id`
   `c3`.`name`
   `c2`.`id`
   `c2`.`name`
   `c3`.`id`
   `c3`.`name`
FROM (
 AS `weight`,
AS `tm_id`,
AS `price`,
 AS `spu_id`,
AS `c3_name`,
AS `c2_id`,
AS `c2_name`,
AS `c1_id`,
AS `c1_name`
`sku_info` `si` JOIN `base_category3` `c3` ON `si`.`category3_id` = `c3`.`id`
JOIN `base_category2` `c2` ON `c3`.`category2_id` = `c2`.`id`
JOIN `base_category1` `c1` ON `c2`.`category1_id` = `c1`.`id`
)
```
# 第5章 实时数据分层
1)实时数仓层级
* 层次->说明->保存位置
* ods->原始数据层->datahub
* dwd->数据明细层，经过清洗、筛选、计算、关联后的数据->datahub
* ads->统计的结果数据->adb mysql
2)实时数仓层级
![Image text](image/尚硅谷实时数据仓库项目(阿里云实时数仓)/8.png)
## 5.1 环境准备
实时计算（Alibaba Cloud Realtime Compute，Powered by Ververica）是阿里云提供的基于Apache Flink构建的企业级大数据计算平台。在PB级别的数据集上可以支持亚秒级别的处理延迟，赋能用户标准实时数据处理流程和行业解决方案；支持DataStream API作业开发，提供了流批统一的Flink SQL，简化BI场景下的开发；可与用户已使用的大数据组件无缝对接，更多增值特性助力企业实时化转型。
购买地址：[https://www.aliyun.com/product/bigdata/sc](https://www.aliyun.com/product/bigdata/sc)
* 购买实时计算服务
* 目前实际开通的就只有独享模式，独享模式分两种包年包月，按量付费(按小时)
* 确认订单
### 5.1.2 创建集群和创建项目
* 开通独享模式控制台（注意:第一次进入开通独享模式控制台，需要授权，弹出页面点击同意授权即可）
* 进入控制台页面
* 新建集群
* 选择订单
* 填写基本信息
* 新建 Bucket
    * 开通对象存储服务器 OSS
    * 配置 Bucket
* 选择默认生成的 vpc，在 Zone 区域选择资源充足的选项即可
* 确认信息
* 设定好后集群，等待集群启动(大概 10 分钟)
* 状态变为运行中以后，点击创建项目，输入:项目名称:stream_project01;项目备注:stream_project01;指定 CU:6
* 然后点击项目列表，可以看到项目创建完成
### 5.1.3 创建工作空间DataWorks
目前实时项目的开发工作已经从原来的实时计算开发平台，迁移到了 Dataworks 中。所以想要进行开发需要回到 Dataworks 的控制台界面。
* 进入 DataWorks 路径:https://workbench.data.aliyun.com/consolenew#/
* 点击创建工作空间
* 输入工作空间名称（不可重复）
* 选择实时计算中的独享模式
* 选择实时计算集群和绑定实时计算项目
* 点击进入数据开发，正式进入 Stream Studio 的页面
### 5.1.4 业务开发平台Stream Studio
* 进入 DataStudio 后，在全部产品页面中切入到 Stream Studio
* 先新建业务流程
## 5.2 ODS到DWD层的业务流程
![Image text](image/尚硅谷实时数据仓库项目(阿里云实时数仓)/9.png)
### 5.2.1 建立任务
* 然后在业务流程下建立任务
* 新建任务节点
* 选择右上角的【切换为 SQL 模式】

> 注意:为什么不能选择 DAG 组件模式，因为标准版中只支持 5 个组件，除去数据源表 和结果表只剩 3 个计算组件一般是不够用的。
如果不使用组件那么就必须使用 FlinkSQL(BLink)来编写业务 SQL。FlinkSQL 开发手册 地址如下: [https://help.aliyun.com/document_detail/111864.html?spm=a2c4g.11186623.6.584.3d736caciztnPF](https://help.aliyun.com/document_detail/111864.html?spm=a2c4g.11186623.6.584.3d736caciztnPF)

### 5.2.2 ODS到DWD层的业务流程
> 需求:将订单信息中支付完成的信息和订单详情进行 Join 操作

![Image text](image/尚硅谷实时数据仓库项目(阿里云实时数仓)/10.png)

* order_info 数据源表

> 注意 1:由于数据都是 DTS 推送过来的，推送过来的字段都在原来的表字段前面加上
了一个 dts_前缀。
注意 2:dts_utc_timestamp、dts_operation_flag、dts_after_flag 是 DataHub 框架提供的字段。dts_utc_timestamp表示时间戳，dts_operation_flag=‘U’表示修改、 ‘I’表示插入、‘D’表示删除，dts_after_flag=‘Y’/‘N’，表示需要修改前的记录还是修改后的记录。
注意 3:dts_order_status=1 表示下单，dts_order_status=2 表示已支付。

```
CREATE TABLE ods_order_info
(
dts_id BIGINT
,dts_user_id BIGINT
,dts_create_time BIGINT
,dts_operate_time BIGINT
,dts_province_id BIGINT
,dts_order_status VARCHAR
,dts_operation_flag VARCHAR
,dts_after_flag VARCHAR ) WITH (
type='datahub'
,project='gmall_datahub'
,topic='order_info' ,endPoint='http://dh-cn-beijing-int-vpc.aliyuncs.com' ,accessId='LTAI4FiU71dZAL17SdLBa6Nt' ,accessKey='63YzSmqMOSjDR5A2ZXEzFLM2tREY6m' ,startTime='2019-10-09 01:15:00'
);
```
* order_detail 数据源表

```
CREATE TABLE ods_order_detail
(
dts_id BIGINT
,dts_order_id BIGINT
,dts_sku_id BIGINT
,dts_sku_name VARCHAR
,dts_sku_num BIGINT
,dts_order_price DOUBLE
,dts_utc_timestamp BIGINT
,dts_operation_flag VARCHAR
,dts_after_flag VARCHAR ) WITH (
type='datahub'
,project='gmall_datahub'
,topic='order_detail' ,endPoint='http://dh-cn-beijing-int-vpc.aliyuncs.com' ,accessId='LTAI4FiU71dZAL17SdLBa6Nt' ,accessKey='63YzSmqMOSjDR5A2ZXEzFLM2tREY6m' ,startTime='2019-10-09 01:15:00'
);
```
* dwd_paid_order_detail 数据结果表

```
CREATE TABLE dwd_paid_order_detail (
   detail_id BIGINT
   ,order_id BIGINT
   ,user_id BIGINT
   ,province_id BIGINT
   ,sku_id BIGINT
   ,sku_name VARCHAR
,sku_num BIGINT
,order_price DOUBLE
,create_time VARCHAR
,pay_time VARCHAR ) WITH (
type='datahub'
,project='gmall_datahub' ,topic='dwd_paid_order_detail' ,endPoint='http://dh-cn-beijing-int-vpc.aliyuncs.com' ,accessId='LTAI4FiU71dZAL17SdLBa6Nt'
,accessKey='63YzSmqMOSjDR5A2ZXEzFLM2tREY6m'
,startTime='2019-10-09 01:15:00' );
```
![Image text](image/尚硅谷实时数据仓库项目(阿里云实时数仓)/11.png)
* 获取 AccessID 和 AccessKey

DataHub 服务并不是靠 IP 来定位的，而是靠阿里云账号，每个阿里云账号只能有一个
DataHub，每个阿里云账号也会有唯一的 AccessID 和 AccessKey。所以通过 AccessId 和 AccessKey 就可以直接锁定某个阿里云账号的 DataHub。

    * 悬浮鼠标到阿里云账号头像上->点击 accesskeys
    * 点击继续使用 AccessKey
    * 新用户需要点击创建 AccessKey
    * 获取到 AccessKeyID 和 AccessKeySecret 值

* 在 DataHub 中创建 Topic(聚合后的结果表 dwd_paid_order_detail)

DataHub 控制台入口地址:https://datahub.console.aliyun.com/datahub/

* 计算插入语句

```
insert into dwd_paid_order_detail select
   od.dts_id
   ,oi.dts_id order_id
,oi.dts_user_id
,oi.dts_province_id
,od.dts_sku_id
,od.dts_sku_name
,od.dts_sku_num
,od.dts_order_price ,FROM_UNIXTIME(cast(oi.dts_create_time/1000000 as BIGINT))
create_time ---业务数据是秒，datahub 中数据是微秒，需要转换 ,FROM_UNIXTIME(cast(oi.dts_operate_time/1000000 as BIGINT))
pay_time
from
(
select * ---实际开发中尽量请写全字段 from ods_order_info
where dts_operation_flag='U' and dts_order_status = '2'
and dts_after_flag = 'Y' ) oi
join (
select *
from ods_order_detail
where dts_operation_flag='I'
) od --双流join
on oi.dts_id = od.dts_order_id;
```

思考：
1、FlinkSQL 是如何解决双流 Join 的问题，会不会出现时间错位而未关联的情况?
FlintkSQL 是一种有状态的流式计算，流中的数据会被以状态的形式保存起来，所
以即使两个流中的数据有一定的延迟，也可以利用状态关联出来。
2、这个状态保存多久?
阿里云中 FlinkSQL 的状态数据默认保存 36 个小时。

### 5.2.3 任务发布与运维
FlinkSQL 编写完成后，点击右侧的执行计划，可以把 FlinkSQL 转化为执行计划图，在 这个过程中 DataWork 会检查 FlinkSQL 的代码，并发现一定的问题。
* 点击执行计划
* 生成执行计划
* 执行计划如果能够成功生成，则可以点击发布按钮
* 发布任务，分配 2 个 CU 资源
* 点击运维页面
* 在运维页面看到发布好的流程可以点击【启动】
* 设置启动点位
* 点击 ods_dwd 任务名称
* 查看运行信息
* 生成当天的测试数据
* 查看 DataHub 中数据

## 5.3 存储结果表数据库
![Image text](image/尚硅谷实时数据仓库项目(阿里云实时数仓)/12.png)

### 5.3.1 AnalyticDB 简介
分析型数据库 MySQL 版(AnalyticDB for MySQL)，是阿里巴巴自主研发的海量数据 实时高并发在线分析(Realtime OLAP)云计算服务，使得您可以在毫秒级针对千亿级数据 进行即时的多维分析透视和业务探索。分析型数据库 MySQL 版对海量数据的自由计算和极 速响应能力，能让用户在瞬息之间进行灵活的数据探索，快速发现数据价值，并可直接嵌入 业务系统为终端客户提供分析服务。
### 5.3.2 AnalyticDB 购买
* 购买地址：https://www.aliyun.com/product/ApsaraDB/ads
* 选择购买配置
* 确认订单
* 开通成功

### 5.3.3 AnalyticDB 添加集群白名单
* 配置实时集群的 IP 地址，加入到维表 RDS 和结果表 AnalyticDB 的白名单中，进入建立集群时的位置->点击集群的实例名->复制 ENI 那列 IP 地址。 说明:阿里实时计算开发平台入口:https://stream.console.aliyun.com/zh/dark/#/profile/cluster
* 添加 ENI 中网址到 AnalyticDB 的白名单中，AnalyticDB 控制台地址:https://ads.console.aliyun.com/adb/cn-beijing/summary
* 点击数据安全->点击添加白名单分组，注意:组内 IP 地址的间隔，只能有一个逗号，多余的空格要删除掉

### 5.3.4 RDS 添加集群白名单
* RDS 控制台地址:https://rdsnext.console.aliyun.com/#/detail/rm-2ze1aqt4524bt8097/security/whiteList?region=cn- beijing
* 点击数据安全性->点击添加白名单分组->输入实时集群的 IP 地址

### 5.3.5 连接 AnalyticDB 并建结果表
* 点击申请外网地址->复制外网地址
* 配置 SQLyog 连接 AnalyticDB，新建连接->SQL 主机地址为 AnalyticDB 的地址->用户名为 root，密码:Atguigu000000
* 创建数据库 gmall_result
* 创建地区分布统计结果表

```
create Table `ads_province_stat` ( `province_id` bigint,
`area_code` varchar,
   `province_name` varchar,
   `region_id` bigint,
   `region_name` varchar,
   `order_amount` varchar,
   `order_count` bigint,
   `dt` varchar,
primary key (province_id,dt) )
DISTRIBUTE BY HASH(`province_id`)
PARTITION BY VALUE(`DATE_FORMAT(dt, '%Y%m')`) LIFECYCLE 24 INDEX_ALL='Y' COMMENT='省份地区销售情况统计'
```
* 创建商品统计结果表

```
create Table `ads_sku_stat` (
   `sku_id` bigint,
   `sku_name` varchar,
   `weight` decimal(16, 2),
   `tm_id` bigint,
`price` decimal(16, 2), `spu_id` bigint,
`c3_id` bigint,
`c3_name` varchar,
`c2_id` bigint,
`c2_name` varchar,
`c1_id` bigint,
`c1_name` varchar, `order_amount` decimal(16, 2), `order_count` bigint, `sku_count` bigint,
   `dt` varchar,
   primary key (sku_id,dt)
)
DISTRIBUTE BY HASH(`sku_id`)
PARTITION BY VALUE(`DATE_FORMAT(dt, '%Y%m')`) LIFECYCLE 24 INDEX_ALL='Y' COMMENT='商品销售情况统计'
```
其中，DISTRIBUTE BY 后面的字段，用于分 Shard，把数据分配到不同的节点上，已达到 数据及负载的均衡。
PARTITION BY 后面的字段用户在节点内进行分区，用于缩小查询时的扫描范围，同时管 理数据生存周期

## 5.4 DWD 到 ADS 层的业务流程(地区销售分布统计)
需求:统计每个地区的销售金额和支付订单数
![Image text](image/尚硅谷实时数据仓库项目(阿里云实时数仓)/13.png)
需要完成的要素:
1. 搭建 analyticDB for Mysql 数据库，用于存放结果表
2. 创建数据源表 dwd_paid_order_detail
3. 创建维表 dim_province
4. 创建结果表 ads_province_stat
5. 对数据源表进行聚合统计，指标 id 关联维表，并写入结果表

![Image text](image/尚硅谷实时数据仓库项目(阿里云实时数仓)/14.png)

### 5.4.1 创建业务流程
* 回到 Stream Studio 控制台地址:https://ss-cn-beijing.data.aliyun.com/
* 新建任务，点击数据开发->点击业务流程->右键新建任务
* 新建任务节点 dwd_ads_province

### 5.4.2 创建数据源表(dwd_paid_order_detail)

```
CREATE TABLE dwd_paid_order_detail ( detail_id BIGINT,
order_id BIGINT,
user_id BIGINT,
province_id BIGINT, sku_id BIGINT, sku_name VARCHAR, sku_num BIGINT, order_price DOUBLE, create_time VARCHAR, pay_time VARCHAR
) WITH (
type='datahub',
project='gmall_datahub', topic='dwd_paid_order_detail', endPoint='http://dh-cn-beijing-int-vpc.aliyuncs.com',
accessId='LTAI4FiU71dZAL17SdLBa6Nt', accessKey='63YzSmqMOSjDR5A2ZXEzFLM2tREY6m', startTime='2019-10-09 01:15:00', batchReadSize='100'
);
```

### 5.4.3 创建维表(dim_province)

```
CREATE TABLE dim_province ( province_id BIGINT,
province_name VARCHAR,
area_code VARCHAR,
region_id BIGINT,
region_name VARCHAR ,
PRIMARY KEY (province_id),
PERIOD FOR SYSTEM_TIME --定义为维表
) WITH (
   type= 'rds',
url = 2zegqj7pcoaj22z9i.mysql.rds.aliyuncs.com:3306/gmall_dim',--您的 数据库 url
tableName = 'dim_province',--您的表名 userName = 'root',--您的用户名 password = 'Atguigu000000',--您的密码
'jdbc:mysql://rm-
   cache = 'lru',
   cacheSize= '1000',
   cacheTTLMs='3600000'
);
```
### 5.4.4 创建结果表(ads_province_stat)

```
CREATE TABLE ads_province_stat
(
   province_id BIGINT
   ,area_code VARCHAR
   ,province_name VARCHAR
   ,region_id BIGINT
   ,region_name VARCHAR
   ,order_amount DOUBLE
   ,order_count BIGINT
   ,dt varchar
,PRIMARY KEY(province_id ,dt ) ) WITH (
   type='ADB30'
,url='jdbc:mysql://am- 2ze9xa90nrnuralq290650.ads.aliyuncs.com:3306/gmall_result'
,tableName='ads_province_stat' ,userName='root' ,password='Atguigu000000'
);
```
### 5.4.5 聚合数据源表和维表
* 对数据源表进行聚合统计，指标 id 关联维表，并写入结果表

```
create view tmp_province_stat
as select
province_id,
mod(order_id,1024) hash_id, count(distinct order_id) order_count, sum(order_price*sku_num) order_amount
from dwd_paid_order_detail
where TO_DATE(pay_time,'yyyy-MM-dd') = CURRENT_DATE group by province_id, mod(order_id,1024);
insert into ads_province_stat
select
pc.province_id
,dp.area_code ,dp.province_name ,dp.region_id
,dp.region_name ,pc.order_amount ,pc.order_count ,cast(CURRENT_DATE as VARCHAR)
from (
   select
       province_id,
sum(order_count) order_count, sum(order_amount) order_amount
   from tmp_province_stat
   group by province_id
)pc
join dim_province FOR SYSTEM_TIME AS OF PROCTIME() as dp on dp.province_id = pc.province_id
```
* 注意:如果是维度表参与 join 操作，都要加上 FOR SYSTEM_TIME AS OF PROCTIME()

### 5.4.6 任务发布与运维
同 5.2.3 步骤

## 5.5 DWD 到 ADS 层的业务流程(商品统计)
需求:统计每个商品的订单个数、订单金额
![Image text](image/尚硅谷实时数据仓库项目(阿里云实时数仓)/15.png)

* 创建数据源表dwd_paid_order_detail
* 创建维表 dim_sku_info
* 创建结果表ads_sku_stat
* 对数据源表进行聚合统计，指标id关联维表，并写入结果表

![Image text](image/尚硅谷实时数据仓库项目(阿里云实时数仓)/16.png)

### 5.5.1 创建业务流程
* 回到 Stream Studio 控制台地址:https://ss-cn-beijing.data.aliyun.com/
* 新建任务，点击数据开发->点击业务流程->右键新建任务
* 新建节点 dwd_ads_sku
### 5.5.2 创建数据源表(dwd_paid_order_detail)

```
CREATE TABLE dwd_paid_order_detail ( detail_id BIGINT,
order_id BIGINT,
user_id BIGINT, province_id BIGINT, sku_id BIGINT, sku_name VARCHAR, sku_num BIGINT, order_price DOUBLE, create_time VARCHAR, pay_time VARCHAR
) WITH (
type='datahub',
project='gmall_datahub', topic='dwd_paid_order_detail', endPoint='http://dh-cn-beijing-int-vpc.aliyuncs.com', accessId='LTAI4FiU71dZAL17SdLBa6Nt', accessKey='63YzSmqMOSjDR5A2ZXEzFLM2tREY6m', startTime='2019-10-09 01:15:00'
);
```
### 5.5.3 创建维表(dim_sku_info)

```
CREATE TABLE dim_sku_info(
   id BIGINT,
   sku_name VARCHAR,
   weight DOUBLE,
   tm_id BIGINT,
   price DOUBLE,
   spu_id BIGINT,
   c3_id BIGINT,
   c3_name VARCHAR ,
   c2_id BIGINT,
   c2_name VARCHAR,
c1_id BIGINT,
c1_name VARCHAR,
PRIMARY KEY (id),
PERIOD FOR SYSTEM_TIME--定义维表的变化周期，表明该表是一张会变化的
表。
) WITH (
   type = 'rds', 
   url = 'jdbc:mysql://rm-2zegqj7pcoaj22z9i.mysql.rds.aliyuncs.com:3306/gmall_dim',--您的 数据库 url
   tableName = 'dim_sku_info',--您的表名 
   userName = 'root',--您的用户名 
   password = 'Atguigu000000'--您的密码
);
```
### 5.5.4 创建结果表(ads_sku_stat)

```
CREATE TABLE ads_sku_stat
(
   sku_id BIGINT,
   sku_name VARCHAR,
   weight DOUBLE,
   tm_id BIGINT,
price DOUBLE,
   spu_id BIGINT,
   c3_id BIGINT,
   c3_name VARCHAR ,
   c2_id BIGINT,
   c2_name VARCHAR,
   c1_id BIGINT,
   c1_name VARCHAR,
   order_amount DOUBLE,
   order_count BIGINT,
   sku_count BIGINT,
dt varchar,
   PRIMARY KEY(sku_id ,dt)
) WITH (
   type='ADB30'
,url='jdbc:mysql://am- 2ze9xa90nrnuralq290650.ads.aliyuncs.com:3306/gmall_result'
,tableName='ads_sku_stat' ,userName='root' ,password='Atguigu000000'
);
```
### 5.5.5 聚合数据源表和维表

```
create view tmp_sku_stat
as select
sku_id,
mod(order_id,1024) hash_id, count(distinct order_id) order_count, sum(order_price*sku_num) order_amount,
sum(sku_num) order_sku_num
from dwd_paid_order_detail
where TO_DATE(pay_time,'yyyy-MM-dd') = CURRENT_DATE group by sku_id, mod(order_id,1024);
```
这里 group by 后面加入 mod(order_id,1024) 是为了避免因为某个 sku 的数据量过大造 成的数据倾斜。

```
insert into ads_sku_stat
select sku_id ,
sku_name ,
weight ,
tm_id ,
price ,
spu_id ,
c3_id ,
c3_name,
c2_id ,
c2_name ,
c1_id ,
c1_name ,
sc.order_amount, sc.order_count , sc.order_sku_num , cast(CURRENT_DATE as VARCHAR)
from (
   select
       sku_id,
sum(order_count) order_count, sum(order_amount) order_amount, sum(order_sku_num) order_sku_num
   from tmp_sku_stat
group by sku_id
)sc join dim_sku_info FOR SYSTEM_TIME AS OF PROCTIME() as ds on ds.id = sc.sku_id
```
### 5.5.6 任务发布与运维
同 5.2.3 步骤。

# 第6章 业务数据准备
## 6.1 DataV 简介
DataV 数据可视化是使用可视化大屏的方式来分析并展示庞杂数据的产品。DataV 能 让更多的人看到数据可视化的魅力，帮助非专业的工程师通过图形化的界面轻松搭建专业水 准的可视化应用，满足您会议展览、业务监控、风险预警、地理信息分析等多种业务的展示 需求。
## 6.2 DataV 和 QuckBI 区别
DataV 更侧重单一屏幕的各种数据的丰富展示效果，更加直观酷炫。适用用于投射大屏。 不注重用户的交互与灵活分析。
QuickBI 侧重于给专业的运营、数据分析师通过多页面的图形报表，进行较为灵活的、 交互性强的维度分析。适用于分析人员利用电脑进行浏览翻页查看。
## 6.3 DataV 实操
### 6.3.1 DataV 购买
* DataV 网站地址:https://data.aliyun.com/visual/datav?spm=5176.10695662.745986.1.9417d81czwDLox&aly_
 as=DmCGMOJS
* 进入产品控制台
* 购买 DataV
* 支付

### 6.3.2 添加数据源
* 点击我的数据->点击添加数据
* 配置读取 AnalyticDB 信息，注意要给需要访问的数据源添加白名单
* 在 AnalyticDB 中添加白名单:
    * AnalyticDB 控制台地址:https://ads.console.aliyun.com/adb/cn-beijing/instances/v3/am-2ze9xa90nrnuralq2/data- security/white/create
    * 外网名单地址:139.224.92.81/24,139.224.92.22/24,139.224.92.35/24,139.224.4.30/24,139.224.92.1 02/24,139.224.4.48/24,139.224.4.104/24,139.224.92.11/24,139.224.4.60/24,139.224.92. 52/24,139.224.4.26/24,139.224.92.57/24,112.74.156.111/24,120.76.104.101/24,139.224. 4.69/24,114.55.195.74/24,47.99.11.181/24,47.94.185.180/24,182.92.144.171/24,139.224 .4.32/24,106.14.210.237/24
### 6.3.3 选择模板
* 点击【新增可视化】，根据业务需要选择对应的模板
* 点击创建
* 创建大屏

### 6.3.4 配置地图模板展示地区分布统计
* 选中中国地图组件，在右侧选中区域热力层
* 组件绑定数据源，选择右侧中间页签下面的【映射数据】
* 配置数据源查询 SQL

```
SELECT area_code area_id, SUM(order_amount) `value` FROM
ads_province_stat td GROUP BY area_code
```
### 6.3.5 配置柱状图模板展示商品统计
* 选择组件后，在右侧的数据想要结果，点击配置数据源
* 配置数据源查询 SQL

```
SELECT c3_name x ,SUM(order_amount) y ,'1' s FROM `ads_sku_stat` GROUP BY c3_name order by y desc
```
注意，返回字段必须是 x（名称），y（数值），s（系列可选）

